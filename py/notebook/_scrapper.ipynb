{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import feedparser\n",
    "from goose3 import Goose\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_url(url):\n",
    "    feed_link = feedparser.parse(url)\n",
    "\n",
    "    link_per_cluster = []\n",
    "    for fl in feed_link[\"items\"]:\n",
    "        f_link = fl[\"summary_detail\"][\"value\"].split('href=\"')[-1].split(' ')[0]\n",
    "        link = f_link.replace(\"google.com/\",\"google.com/news/rss/\")\n",
    "        link_per_cluster.append(link)\n",
    "\n",
    "    link_per_cluster_all = []\n",
    "    for i in tqdm(range(len(link_per_cluster))):\n",
    "        lpc = link_per_cluster[i]\n",
    "        f_link = feedparser.parse(lpc)\n",
    "        for fl in f_link['items']:\n",
    "            title = fl['title']\n",
    "            link = fl['links'][0]['href']\n",
    "            host = link.split('//')[1].split('/')[0]\n",
    "            publish_date = datetime.strptime(fl['published'], \"%a, %d %b %Y %H:%M:%S GMT\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            link_per_cluster_all.append((title,link,host,publish_date))\n",
    "\n",
    "    return link_per_cluster_all\n",
    "\n",
    "def get_all_url():\n",
    "    category = [['WORLD','Dunia'],['NATION','Indonesia'],['BUSINESS','Bisnis'],['ENTERTAINMENT','Hiburan'],\n",
    "            ['TECHNOLOGY','Teknologi'],['SPORTS','Olahraga'],['SCIENCE','Science'],['HEALTH','Kesehatan']]\n",
    "    \n",
    "    all_url = []\n",
    "    for c,y in category:\n",
    "        print(y)\n",
    "        url = \"\"\"https://news.google.com/news/rss/headlines/section/topic/{category}.id_id/Indonesia?ned=id_id&hl=id&gl=ID\"\"\".format(category=c)\n",
    "        res = get_title_url(url)\n",
    "        fin_res = {\n",
    "            \"category\": y,\n",
    "            \"items\": res\n",
    "        }\n",
    "        all_url.append(fin_res)\n",
    "\n",
    "    return all_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dunia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indonesia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bisnis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiburan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:18<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teknologi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olahraga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kesehatan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "all_url = get_all_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baca(text):\n",
    "    baca_word = ['Informasi Menarik Terbaru','Membaca:','Baca juga','Baca :','BACA JUGA:','Penulis :','Penulis: ',\n",
    "                'Artikel ini telah tayang di','Baca:','BACA :','Baca Juga:','Baca artikel sumber','Baca Selengkapnya:',\n",
    "                 'Simak pula video pilihan berikut:','Saksikan tayangan video menarik berikut ini:',\n",
    "                 'Saksikan Video Pilihan Berikut Ini:']\n",
    "\n",
    "    junk_word = []\n",
    "    for word in baca_word:\n",
    "        for i in text.split('.'):\n",
    "            if i.find(word) >= 0 :\n",
    "                junk_word.append(i)\n",
    "\n",
    "    for j in junk_word:\n",
    "        text = text.replace(j, ' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "def brut_split(text):\n",
    "    for alf in list(string.ascii_uppercase):\n",
    "        text = text.replace('.{a}'.format(a=alf),'. {a}'.format(a=alf))\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_publisher(text):\n",
    "    pre = text[:100]\n",
    "    suf = text[100:]\n",
    "    try:\n",
    "        try:\n",
    "            final = pre.split(' - ')[1] + suf\n",
    "            return final\n",
    "        except:\n",
    "            final = pre.split(' -')[1] + suf\n",
    "            return final\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(link):\n",
    "    g = Goose({\n",
    "                'use_meta_language': False, \n",
    "                'target_language':'id',\n",
    "                'enable_image_fetching': False,\n",
    "            })\n",
    "    extract = g.extract(url=link)\n",
    "\n",
    "    content = extract.cleaned_text\n",
    "    content = remove_publisher(content)\n",
    "    content = content.replace('.\"','. ')\n",
    "    content = content.replace('\\n',' ').replace('   ',' ').replace('  ',' ').replace(\"\\'\",\"\").strip('-').strip()\n",
    "    content = re.sub(r'[^\\x00-\\x7F]+', '', content)\n",
    "    content = content.replace(' ...','.').replace('.. .','. ')\n",
    "    content = brut_split(content)\n",
    "    content = content.replace('.CO','').replace('.COM','').replace('. CO','').replace('. COM','')\n",
    "    content = remove_baca(content)\n",
    "    spoiler = content[:150] + '...'\n",
    "\n",
    "    if len(content) <= 500:\n",
    "        return \"Not Valid\"\n",
    "    else:\n",
    "        return content, spoiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM Tagger & Entity Determiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "# import string\n",
    "# from nltk import sent_tokenize\n",
    "# from collections import defaultdict\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "# from nltk import word_tokenize, RegexpParser, ne_chunk\n",
    "# from nltk.tag.hmm  import HiddenMarkovModelTagger, HiddenMarkovModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('my_tagger.dill', 'rb') as f:\n",
    "#     hmm_tagger = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hasNumbers(inputString):\n",
    "#     result = False\n",
    "#     for char in list(inputString):\n",
    "#         if(char.isdigit()):\n",
    "#             result = True\n",
    "#     return result\n",
    "\n",
    "# def xcheck_tag(word,tag):\n",
    "#     punc = list(string.punctuation)\n",
    "    \n",
    "#     dates = ['Januari','Februari','Maret','April','Mei','Juni','Juli','Agustus','September','Oktober','November','Desember',\\\n",
    "#             'Jan','Feb','Mar','Apr','Mei','Jun','Jul','Agt','Sep','Okt','Nov','Des',\\\n",
    "#             'Senin','Selasa','Rabu','Kamis','Jumat','Sabtu','Minggu'\n",
    "#         ]\n",
    "#     stopword = open('stopword.txt','r').read().split('\\n')\n",
    "    \n",
    "#     if(word.lower() in stopword):\n",
    "#         tag = 'STOPWORD'\n",
    "    \n",
    "#     if(word in dates):\n",
    "#         tag = 'DATE'\n",
    "    \n",
    "#     if(word in punc):\n",
    "#         tag = 'Z'\n",
    "        \n",
    "#     if(tag == 'CD' and word.isdigit()):\n",
    "#         tag = 'CD'\n",
    "        \n",
    "#     if(tag in ['SYM','Z','CD','MD'] and word.upper() != word and hasNumbers(word) == False \\\n",
    "#       and word[-3:] not in ['nya','kah','lah']):\n",
    "#         tag = 'NNP'\n",
    "    \n",
    "#     if(tag == 'NN' and word[:1].upper() == word):\n",
    "#         tag = 'NNP'\n",
    "        \n",
    "#     if(tag == 'NNP' and word.lower() == word):\n",
    "#         tag = 'NN'\n",
    "    \n",
    "#     if(tag == 'NNP' and len(word) == 1):\n",
    "#         tag = 'NN'\n",
    "        \n",
    "#     if(tag == 'FW' and word.lower() == word):\n",
    "#         tag = 'NN'\n",
    "        \n",
    "#     return word,tag\n",
    "\n",
    "# def get_entity(wording, verbose=False):\n",
    "#     word_punct_tokenizer = WordPunctTokenizer()\n",
    "#     tokenized = word_punct_tokenizer.tokenize(wording)\n",
    "#     final_tagged = []\n",
    "#     pos_tagging_begin = hmm_tagger.tag(tokenized)\n",
    "#     for ptb in pos_tagging_begin:\n",
    "#         w,t = xcheck_tag(ptb[0],ptb[1])\n",
    "#         final_tagged.append((w,t))\n",
    "    \n",
    "#     if(verbose):\n",
    "#         print(pos_tagging_begin)\n",
    "    \n",
    "#     grammar = \"\"\"\n",
    "#     ENTITY : {<NNP>+}\n",
    "#     ENTITY : {<FW>+}\n",
    "#     \"\"\"\n",
    "#     result = []\n",
    "#     chunkParser = RegexpParser(grammar)\n",
    "#     tree = chunkParser.parse(final_tagged)\n",
    "#     for subtree in tree.subtrees():\n",
    "#         if(subtree.label()==\"ENTITY\"):\n",
    "#             tampung_entity = []\n",
    "#             for se in subtree.leaves():\n",
    "#                 tampung_entity.append(se[0])\n",
    "#             result.append(' '.join(tampung_entity))\n",
    "#     return result\n",
    "\n",
    "# def sorting(xsz, top=10):\n",
    "#     counts = defaultdict(int)\n",
    "#     for xs in xsz:\n",
    "#         for x in xs:\n",
    "#             counts[x] += 1\n",
    "#     return sorted(counts.items(), reverse=True, key=lambda tup: tup[1])[:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for l in all_url[1:2]:\n",
    "#     final_result = []\n",
    "#     print(l['category'])\n",
    "#     print()\n",
    "    \n",
    "#     for item in l['items'][:1]:\n",
    "#         print(item)\n",
    "#         print()\n",
    "#         title = item[0]\n",
    "#         link = item[1]\n",
    "#         content = get_content(link)\n",
    "#         print(title)\n",
    "#         print()\n",
    "#         print(link)\n",
    "#         print()\n",
    "#         print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# senttok = sent_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# all_entity = []\n",
    "# for xsent in senttok:\n",
    "#     if len(xsent) > 1:\n",
    "#         ent = get_entity(xsent, verbose=False)\n",
    "#         all_entity.append(ent)\n",
    "        \n",
    "#         xsent_final = xsent\n",
    "#         duplicate_word = []\n",
    "#         for e in ent:\n",
    "#             if e in duplicate_word:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 duplicate_word.append(e)\n",
    "#                 xsent_final = xsent_final.replace(e, \"<mark>{e}</mark>\".format(e=e))\n",
    "            \n",
    "#         print(\"<p>\" + xsent_final + \"</p>\")\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorting(all_entity,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import sent_tokenize\n",
    "nlp = spacy.load('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import sent_tokenize\n",
    "nlp = spacy.load('id')\n",
    "\n",
    "def edit_tagged_content(content):\n",
    "    senttok = sent_tokenize(content)\n",
    "    \n",
    "    for i in range(len(senttok)):\n",
    "        try:\n",
    "            s = senttok[i]\n",
    "            if s.count('\"') == 1:\n",
    "                senttok[i] = s + ' ' + senttok[i + 1]\n",
    "                del senttok[i + 1]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    result_text = []\n",
    "    result_ent = []\n",
    "    result_ent_label = []\n",
    "    for xsent in senttok:\n",
    "        if len(xsent) > 1:\n",
    "            doc = nlp(xsent)\n",
    "\n",
    "            entity = []\n",
    "            all_ent = []\n",
    "            for e in doc.ents:\n",
    "                entity.append((e.text,e.label_))\n",
    "                all_ent.append(e.text)\n",
    "\n",
    "            xsent_final = xsent\n",
    "            duplicate_word = []\n",
    "\n",
    "            for ent, tag in entity:\n",
    "                if ent in duplicate_word:\n",
    "                    pass\n",
    "                else:\n",
    "                    duplicate_word.append(ent)\n",
    "                    xsent_final = xsent_final.replace(ent, '<mark class=\"mark {tag}\">{ent}<span class=\"tag\">{tag}</span></mark>'.format(ent=ent, tag=tag))\n",
    "\n",
    "\n",
    "            result_text.append(\"<p>\" + xsent_final + \"</p>\")\n",
    "            result_ent.append(all_ent)\n",
    "            result_ent_label.append(entity)\n",
    "    return (result_text,result_ent, result_ent_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MySQLdb as mdb\n",
    "\n",
    "db_host = '127.0.0.1'\n",
    "db_user = 'root'\n",
    "db_password = 'qwerty'\n",
    "db_name = 'entity_determiner'\n",
    "db_charset = 'utf8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "science = all_url[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'Science',\n",
       " 'items': [('Bakal Ada Gerhana Bulan Total Tanggal 28 Juli 2018, Simak 6 Tips Memotret Blood Moon Pakai Ponsel',\n",
       "   'http://jatim.tribunnews.com/2018/07/19/bakal-ada-gerhana-bulan-total-tanggal-28-juli-2018-simak-6-tips-memotret-blood-moon-pakai-ponsel',\n",
       "   'jatim.tribunnews.com',\n",
       "   '2018-07-19 11:07:35'),\n",
       "  ('Gerhana Bulan Total 28 Juli 2018, Catat Waktu Proses Awal Hingga Puncaknya',\n",
       "   'http://jambi.tribunnews.com/2018/07/19/gerhana-bulan-total-28-juli-2018-catat-waktu-proses-awal-hingga-puncaknya',\n",
       "   'jambi.tribunnews.com',\n",
       "   '2018-07-19 10:25:37'),\n",
       "  ('Info BMKG - Ini Proses Gerhana Bulan Total Blood Moon 28 Juli 2018, Perhatikan Waktu Puncaknya!',\n",
       "   'http://makassar.tribunnews.com/2018/07/19/info-bmkg-ini-proses-gerhana-bulan-total-blood-moon-28-juli-2018-perhatikan-waktu-puncaknya',\n",
       "   'makassar.tribunnews.com',\n",
       "   '2018-07-19 09:37:00'),\n",
       "  ('Prediksi GBT 28 Juli Jadi yang Terlama Sepanjang Abad 21, BMKG akan Siarkan Secara Langsung',\n",
       "   'http://jateng.tribunnews.com/2018/07/19/prediksi-gbt-28-juli-jadi-yang-terlama-sepanjang-abad-21-bmkg-akan-siarkan-secara-langsung',\n",
       "   'jateng.tribunnews.com',\n",
       "   '2018-07-19 09:16:08'),\n",
       "  ('6 Tips Motret Gerhana Bulan 28 Juli 2018 Pakai Ponsel',\n",
       "   'http://makassar.tribunnews.com/2018/07/19/6-tips-motret-gerhana-bulan-28-juli-2018-pakai-ponsel',\n",
       "   'makassar.tribunnews.com',\n",
       "   '2018-07-19 09:12:00'),\n",
       "  ('Tak hanya Shalat Sunnah, Berikut 8 Amalan yang Sebaiknya Dilakukan saat Gerhana Bulan 28 Juli Nanti',\n",
       "   'http://palembang.tribunnews.com/2018/07/19/tak-hanya-shalat-sunnah-berikut-8-amalan-yang-sebaiknya-dilakukan-saat-gerhana-bulan-28-juli-nanti',\n",
       "   'palembang.tribunnews.com',\n",
       "   '2018-07-19 08:09:00'),\n",
       "  ('BMKG: Gerhana Bulan Total 28 Juli Peristiwa Langka',\n",
       "   'https://kumparan.com/@kumparannews/bmkg-gerhana-bulan-total-28-juli-peristiwa-langka-27431110790549293',\n",
       "   'kumparan.com',\n",
       "   '2018-07-19 09:59:09'),\n",
       "  ('Jadi yang Terlama Abad ini, Begini Tips Memotret Gerhana Bulan Total Blood Moon Lewat Ponsel Android',\n",
       "   'http://jatim.tribunnews.com/2018/07/19/jadi-yang-terlama-abad-ini-begini-tips-memotret-gerhana-bulan-total-blood-moon-lewat-ponsel-android',\n",
       "   'jatim.tribunnews.com',\n",
       "   '2018-07-19 07:43:00'),\n",
       "  ('7 Cara Memotret Gerhana Bulan Total, Super Blood Moon 28 Juli dengan Kamera Handphone',\n",
       "   'http://medan.tribunnews.com/2018/07/19/7-cara-memotret-gerhana-bulan-total-super-blood-moon-28-juli-dengan-kamera-handphone',\n",
       "   'medan.tribunnews.com',\n",
       "   '2018-07-19 07:11:00'),\n",
       "  ('Gerhana Bulan Super Blood Moon 28 Juli 2018, Ini 7 Adab Islam untuk Menyambutnya',\n",
       "   'http://medan.tribunnews.com/2018/07/19/gerhana-bulan-super-blood-moon-28-juli-2018-ini-7-adab-islam-untuk-menyambutnya',\n",
       "   'medan.tribunnews.com',\n",
       "   '2018-07-19 06:35:36'),\n",
       "  ('Gerhana Bulan Total Terjadi di Indonesia 28 Juli, Terlama di Abad 21',\n",
       "   'https://www.liputan6.com/news/read/3594024/gerhana-bulan-total-terjadi-di-indonesia-28-juli-terlama-di-abad-21',\n",
       "   'www.liputan6.com',\n",
       "   '2018-07-19 08:06:00'),\n",
       "  ('Dari Mana Datangnya Pasir Pantai? Sebagian Ternyata dari Usus Ikan Ini',\n",
       "   'http://www.infomenarik-terbaru.com/dari-mana-datangnya-pasir-pantai-sebagian-ternyata-dari-usus-ikan-ini/',\n",
       "   'www.infomenarik-terbaru.com',\n",
       "   '2018-07-19 13:16:00'),\n",
       "  ('Video Ini Rekam Kupu-kupu Minum Air Mata Kura-kura',\n",
       "   'https://sains.kompas.com/read/2018/07/19/193200323/video-ini-rekam-kupu-kupu-minum-air-mata-kura-kura',\n",
       "   'sains.kompas.com',\n",
       "   '2018-07-19 12:33:25'),\n",
       "  ('Bayi Ular Ditemukan Terjebak di Dalam Batu Ambar Selama 99 Juta Tahun',\n",
       "   'https://www.liputan6.com/global/read/3594443/bayi-ular-ditemukan-terjebak-di-dalam-batu-ambar-selama-99-juta-tahun',\n",
       "   'www.liputan6.com',\n",
       "   '2018-07-19 12:46:52'),\n",
       "  ('Hanya untuk Berselfi, Wisatawan Membandel Terobos Masuk Waterblow Nusa Dua saat Embusan Ombak Besar',\n",
       "   'http://bali.tribunnews.com/2018/07/19/hanya-untuk-berselfi-wisatawan-membandel-terobos-masuk-waterblow-nusa-dua-saat-embusan-besar',\n",
       "   'bali.tribunnews.com',\n",
       "   '2018-07-19 11:52:54')]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:24<00:00,  1.62s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:20<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for au in science:\n",
    "    news = science['items']\n",
    "    save_to_db = []\n",
    "    for i in tqdm(range(len(news))):\n",
    "        d = news[i]\n",
    "        try:\n",
    "            title, url, host, published_at = d\n",
    "            clean_content, spoiler_content = get_content(url)\n",
    "            tagged_content, entity, entity_label = edit_tagged_content(clean_content)\n",
    "            tagged_content = '\\n'.join(tagged_content)\n",
    "            \n",
    "            final_ent = []\n",
    "            for ent in entity:\n",
    "                for e in ent:\n",
    "                    final_ent.append(e)\n",
    "            entity = '*'.join(final_ent)\n",
    "            \n",
    "            final_ent_label = []\n",
    "            for entl in entity_label:\n",
    "                for ent in entl:\n",
    "                    final_ent_label.append(ent[0] + '#')\n",
    "                    final_ent_label.append(ent[1] + '*')\n",
    "                    \n",
    "            entity_label = ''.join(final_ent_label)\n",
    "            save_to_db.append((title, clean_content, tagged_content, spoiler_content, entity, entity_label, url, host, published_at))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(\"Total data : %d\" % len(save_to_db))\n",
    "    query = \"\"\"INSERT IGNORE INTO {table}\n",
    "        (title, clean_content, tagged_content, spoiler_content, entity, entity_label, url, host, published_at)\n",
    "        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\"\".format(table=science['category'].lower())\n",
    "    attr = tuple(save_to_db)\n",
    "\n",
    "    connect = mdb.connect(db_host, db_user, db_password, db_name, charset=db_charset)\n",
    "    cursor = connect.cursor()\n",
    "    cursor.executemany(query, attr)\n",
    "    connect.commit()\n",
    "    connect.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global-ENV",
   "language": "python",
   "name": "global-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import feedparser\n",
    "from goose3 import Goose\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_url(url):\n",
    "    feed_link = feedparser.parse(url)\n",
    "\n",
    "    link_per_cluster = []\n",
    "    for fl in feed_link[\"items\"]:\n",
    "        f_link = fl[\"summary_detail\"][\"value\"].split('href=\"')[-1].split(' ')[0]\n",
    "        link = f_link.replace(\"google.com/\",\"google.com/news/rss/\")\n",
    "        link_per_cluster.append(link)\n",
    "\n",
    "    link_per_cluster_all = []\n",
    "    for i in tqdm(range(len(link_per_cluster))):\n",
    "        lpc = link_per_cluster[i]\n",
    "        f_link = feedparser.parse(lpc)\n",
    "        for fl in f_link['items']:\n",
    "            title = fl['title']\n",
    "            link = fl['links'][0]['href']\n",
    "            host = link.split('//')[1].split('/')[0]\n",
    "            publish_date = datetime.strptime(fl['published'], \"%a, %d %b %Y %H:%M:%S GMT\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            link_per_cluster_all.append((title,link,host,publish_date))\n",
    "\n",
    "    return link_per_cluster_all\n",
    "\n",
    "def get_all_url():\n",
    "    category = [['WORLD','Dunia'],['NATION','Indonesia'],['BUSINESS','Bisnis'],['ENTERTAINMENT','Hiburan'],\n",
    "            ['TECHNOLOGY','Teknologi'],['SPORTS','Olahraga'],['SCIENCE','Science'],['HEALTH','Kesehatan']]\n",
    "    \n",
    "    all_url = []\n",
    "    for c,y in category[:1]:\n",
    "        print(y)\n",
    "        url = \"\"\"https://news.google.com/news/rss/headlines/section/topic/{category}.id_id/Indonesia?ned=id_id&hl=id&gl=ID\"\"\".format(category=c)\n",
    "        res = get_title_url(url)\n",
    "        fin_res = {\n",
    "            \"category\": y,\n",
    "            \"items\": res\n",
    "        }\n",
    "        all_url.append(fin_res)\n",
    "\n",
    "    return all_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dunia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "all_url = get_all_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baca(text):\n",
    "    baca_word = ['Informasi Menarik Terbaru','Membaca:','Baca juga','Baca :','BACA JUGA:','Penulis :','Penulis: ',\n",
    "                'Artikel ini telah tayang di','Baca:','BACA :','Baca Juga:','Baca artikel sumber','Baca Selengkapnya:',\n",
    "                 'Simak pula video pilihan berikut:','Saksikan tayangan video menarik berikut ini:',\n",
    "                 'Saksikan Video Pilihan Berikut Ini:']\n",
    "\n",
    "    junk_word = []\n",
    "    for word in baca_word:\n",
    "        for i in text.split('.'):\n",
    "            if i.find(word) >= 0 :\n",
    "                junk_word.append(i)\n",
    "\n",
    "    for j in junk_word:\n",
    "        text = text.replace(j, ' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "def brut_split(text):\n",
    "    for alf in list(string.ascii_uppercase):\n",
    "        text = text.replace('.{a}'.format(a=alf),'. {a}'.format(a=alf))\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_publisher(text):\n",
    "    pre = text[:100]\n",
    "    suf = text[100:]\n",
    "    try:\n",
    "        try:\n",
    "            final = pre.split(' - ')[1] + suf\n",
    "            return final\n",
    "        except:\n",
    "            final = pre.split(' -')[1] + suf\n",
    "            return final\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(link):\n",
    "    g = Goose({\n",
    "                'use_meta_language': False, \n",
    "                'target_language':'id',\n",
    "                'enable_image_fetching': True,\n",
    "            })\n",
    "    extract = g.extract(url=link)\n",
    "    \n",
    "    content = extract.cleaned_text\n",
    "    content = remove_publisher(content)\n",
    "    content = content.replace('.\"','. ')\n",
    "    content = content.replace('\\n',' ').replace('   ',' ').replace('  ',' ').replace(\"\\'\",\"\").strip('-').strip()\n",
    "    content = re.sub(r'[^\\x00-\\x7F]+', '', content)\n",
    "    content = content.replace(' ...','.').replace('.. .','. ')\n",
    "    content = brut_split(content)\n",
    "    content = content.replace('.CO','').replace('.COM','').replace('. CO','').replace('. COM','')\n",
    "    content = remove_baca(content)\n",
    "    spoiler = content[:150] + '...'\n",
    "    image = extract.top_image\n",
    "    image_src = image.src\n",
    "\n",
    "    if len(content) <= 500:\n",
    "        return \"Not Valid\"\n",
    "    else:\n",
    "        return content, spoiler, image_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulaziz/Skripsi/apps/entity-determiner/.env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/abdulaziz/Skripsi/apps/entity-determiner/.env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/Users/abdulaziz/Skripsi/apps/entity-determiner/.env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/abdulaziz/Skripsi/apps/entity-determiner/.env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import sent_tokenize\n",
    "nlp = spacy.load('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import sent_tokenize\n",
    "nlp = spacy.load('id')\n",
    "\n",
    "def edit_tagged_content(content):\n",
    "    senttok = sent_tokenize(content)\n",
    "    \n",
    "    for i in range(len(senttok)):\n",
    "        try:\n",
    "            s = senttok[i]\n",
    "            if s.count('\"') == 1:\n",
    "                senttok[i] = s + ' ' + senttok[i + 1]\n",
    "                del senttok[i + 1]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    result_text = []\n",
    "    result_ent = []\n",
    "    result_ent_label = []\n",
    "    for xsent in senttok:\n",
    "        if len(xsent) > 1:\n",
    "            doc = nlp(xsent)\n",
    "\n",
    "            entity = []\n",
    "            all_ent = []\n",
    "            for e in doc.ents:\n",
    "                entity.append((e.text,e.label_))\n",
    "                all_ent.append(e.text)\n",
    "\n",
    "            xsent_final = xsent\n",
    "            duplicate_word = []\n",
    "\n",
    "            for ent, tag in entity:\n",
    "                if ent in duplicate_word:\n",
    "                    pass\n",
    "                else:\n",
    "                    duplicate_word.append(ent)\n",
    "                    xsent_final = xsent_final.replace(ent, '<mark class=\"mark {tag}\">{ent}<span class=\"tag\">{tag}</span></mark>'.format(ent=ent, tag=tag))\n",
    "\n",
    "\n",
    "            result_text.append(\"<p>\" + xsent_final + \"</p>\")\n",
    "            result_ent.append(all_ent)\n",
    "            result_ent_label.append(entity)\n",
    "    return (result_text,result_ent, result_ent_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MySQLdb as mdb\n",
    "\n",
    "db_host = '127.0.0.1'\n",
    "db_user = 'root'\n",
    "db_password = 'qwerty'\n",
    "db_name = 'entity_determiner'\n",
    "db_charset = 'utf8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:24<00:00,  1.62s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:20<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for au in all_url:\n",
    "    news = au['items']\n",
    "    save_to_db = []\n",
    "    for i in tqdm(range(len(news))):\n",
    "        d = news[i]\n",
    "        try:\n",
    "            title, url, host, published_at = d\n",
    "            clean_content, spoiler_content, img_url = get_content(url)\n",
    "            tagged_content, entity, entity_label = edit_tagged_content(clean_content)\n",
    "            tagged_content = '\\n'.join(tagged_content)\n",
    "            \n",
    "            final_ent = []\n",
    "            for ent in entity:\n",
    "                for e in ent:\n",
    "                    final_ent.append(e)\n",
    "            entity = '*'.join(final_ent)\n",
    "            \n",
    "            final_ent_label = []\n",
    "            for entl in entity_label:\n",
    "                for ent in entl:\n",
    "                    final_ent_label.append(ent[0] + '#')\n",
    "                    final_ent_label.append(ent[1] + '*')\n",
    "                    \n",
    "            entity_label = ''.join(final_ent_label)\n",
    "            save_to_db.append((title, clean_content, tagged_content, spoiler_content, entity, entity_label, url, host, published_at, img_url))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(\"Total data : %d\" % len(save_to_db))\n",
    "    query = \"\"\"INSERT IGNORE INTO {table}\n",
    "        (title, clean_content, tagged_content, spoiler_content, entity, entity_label, url, host, published_at, img_url)\n",
    "        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\"\".format(table=science['category'].lower())\n",
    "    attr = tuple(save_to_db)\n",
    "\n",
    "    connect = mdb.connect(db_host, db_user, db_password, db_name, charset=db_charset)\n",
    "    cursor = connect.cursor()\n",
    "    cursor.executemany(query, attr)\n",
    "    connect.commit()\n",
    "    connect.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
